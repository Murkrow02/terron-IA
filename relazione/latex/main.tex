\documentclass[12pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{natbib}
\bibliographystyle{ieeetr}
\setcitestyle{super,open={[},close={]}}

\title{Fine-Tuning Large Language Models on Consumer Hardware Using ILAB}
\author{Marco Coppola,\\
Valerio Pio De Nicola,\\
Davide De Rosa \\ \\
University of Bologna}
\date{}

\begin{document}

\maketitle

\section{Introduction}
Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by demonstrating remarkable capabilities across a wide range of tasks. However, training and fine-tuning these models often require extensive computational resources, making them inaccessible to researchers and developers with limited hardware capabilities. In this study, we explore the feasibility of fine-tuning LLMs on consumer hardware using ILAB, an open-source framework designed to optimize instruction tuning through synthetic data generation and efficient training techniques. Our objective is to assess the effectiveness of this approach and provide insights into its practical implementation. Specifically, we investigate the role of model quantization, low-rank adaptation, and synthetic dataset generation in enabling large-scale model customization with constrained computational resources.

\section{InstructLab}
\subsection{Introduction to LAB}
Large language models (LLMs) have achieved success in NLP tasks due to the transformer architecture. LLM training consists of two main phases: \textbf{pre-training} (which is computationally expensive and involves predicting the next token using massive datasets) and \textbf{alignment tuning} (which refines model behavior through instruction and preference tuning).\vspace{14pt}\\
Pre-training dominates the resource cost, while alignment tuning, including instruction tuning (training on task-specific instructions) and preference tuning (using human feedback methods like RLHF), requires significantly fewer data and compute resources.\\
Despite this, alignment tuning is crucial for optimizing LLMs for real-world use.\vspace{14pt}\\
To address challenges in scaling alignment tuning, the \textbf{LAB} (\textbf{Large-scale Alignment for chatBots}\cite{sudalairaj2024lablargescalealignmentchatbots}) \textbf{method} has been recently created. LAB includes:
\begin{enumerate}
    \item \textbf{Synthetic data generation} guided by taxonomy and quality assurance, avoiding reliance on proprietary LLMs or extensive human curation.  
    \item \textbf{A novel multi-phase training framework} that integrates new knowledge without causing catastrophic forgetting.
\end{enumerate}
LAB-trained models achieve competitive performance compared to those using human-annotated or GPT-4-generated data, demonstrating its effectiveness for improving LLM instruction-following capabilities.\vspace{14pt}\\
Recent advancements in instruction tuning for large language models (LLMs) have primarily relied on two key methodologies: \textbf{human-annotated datasets} and \textbf{synthetic data generation}. Traditional approaches, such as those pioneered by OpenAI and later adopted by the creators of LLaMA 2, emphasize the collection of high-quality human-generated data. This process involves extensive human annotation efforts, requiring rigorous selection and training of annotators to ensure consistency and quality.\vspace{14pt}\\
While effective in aligning models with human preferences, these methods are resource-intensive, costly, and often slow, limiting the ability to rapidly explore new instruction types and model capabilities.
To address these limitations, synthetic data generation has emerged as an alternative, leveraging LLMs to produce instruction-tuning datasets.\vspace{14pt}\\
Early efforts, such as Self-Instruct, introduced a bootstrapping approach that expands a small set of human-written seed instructions into a large dataset using an LLM’s own generation capabilities. Subsequent refinements sought to enhance data diversity and instruction complexity through iterative and principled augmentation techniques.\\
Other works have further extended this approach by focusing on task diversity and progressive training frameworks, incorporating richer reasoning signals to improve model performance incrementally.\vspace{14pt}\\
More recently, semi-automated techniques, such as GLAN, have introduced a structured approach to synthetic data generation by leveraging human-curated taxonomies.\\
However, such methods remain constrained by the limitations of the teacher model used for data generation, particularly when relying on proprietary models like GPT-4, which impose restrictions on the commercial usability of the generated data.\vspace{14pt}\\
In contrast, open-source approaches, such as LAB, attempt to mitigate these concerns by employing models like Mixtral, allowing for greater flexibility in data generation and application.\vspace{14pt}\\
The ongoing evolution of instruction tuning methods highlights the trade-offs between human-annotated and synthetic data-driven approaches.\\
While human-generated data ensures high-quality alignment, it is costly and inflexible.\\
Conversely, synthetic data techniques offer scalability and efficiency but introduce challenges related to data diversity, quality control, and legal constraints surrounding proprietary models.\\
Future research will likely focus on hybrid approaches that balance these trade-offs, incorporating the strengths of both methodologies to enhance the instruction-tuning process.

\subsection{How does LAB work}
LAB is a structured framework designed to enhance instruction tuning through a combination of systematic data curation and multi-phased training. It consists of two key components:
\begin{itemize}
    \item \textbf{Taxonomy-Guided Synthetic Data Generator}: this component facilitates data curation by organizing instructions into a structured taxonomy. It also plays a crucial role in guiding the synthetic data generation process, ensuring both high diversity and quality in the instruction-tuning dataset. By leveraging a well-defined taxonomy, LAB aims to improve the comprehensiveness and relevance of the generated data.
    \item \textbf{Multi-Phased Instruction-Tuning with Replay Buffers}: to maintain training stability and mitigate issues such as catastrophic forgetting, LAB employs a multi-stage tuning approach. The use of replay buffers allows the model to retain previously learned information while adapting to new instructions, thereby improving overall alignment performance.
\end{itemize}
Together, these components form an end-to-end pipeline for aligning a pre-trained LLM, as illustrated in Figure~\ref{fig:ilab}. This approach balances synthetic data diversity with training stability, making it a scalable and effective method for large-scale instruction tuning.
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{img/figure1.png}
    \caption{\textit{Overview of the LAB alignment method. Starting from the taxonomy root, data are curated in each top-level groups and examples in the leaf nodes are used by the synthetic data generators to generate orders of magnitude data for the phased-training step for instruct-tuning.}}
    \label{fig:ilab}
\end{figure}

\subsubsection{Taxonmy}
LAB's taxonomy serves as a structured framework for organizing instruction-tuning data, enabling systematic curation and enhancement of training datasets. It hierarchically classifies data samples into three main branches: \textbf{knowledge}, \textbf{foundational skills}, and \textbf{compositional skills}. Each of these branches is further subdivided into more granular levels, with specific tasks defined at the leaf nodes. These leaf nodes are exemplified by manually written instruction-response pairs, ensuring clarity in task representation.\vspace{14pt}\\
This hierarchical organization provides several advantages.\vspace{14pt}\\
First, it allows model designers and data curators to identify gaps in the model’s capabilities by pinpointing missing or underrepresented tasks.\vspace{14pt}\\
Second, it facilitates the incremental expansion of training data, as new tasks can be seamlessly integrated by adding a leaf node under the appropriate branch, accompanied by 1–3 examples.\vspace{14pt}\\
This structured approach ensures that instruction tuning remains both comprehensive and adaptable, improving the overall alignment and robustness of the trained LLM.\vspace{14pt}\\
\textbf{Knowledge}\\
The \textit{Knowledge} branch categorizes domain-specific documents like textbooks and research papers. It ensures ethical synthetic data generation by selecting only licensed content.\vspace{14pt}\\ 
\textbf{Foundational Skills}\\
The \textit{Foundational Skills} branch includes core abilities like math, coding, and reasoning. Public datasets support structured learning for effective generalization.\vspace{14pt}\\
\textbf{Compositional Skills}\\
The \textit{Compositional Skills} branch integrates knowledge and foundational skills for complex tasks. It enables models to synthesize information for coherent, multi-step responses.

\subsubsection{Taxonomy-Driven Synthetic Data Generator}\label{tax-sdg}
LAB enhances \textbf{synthetic data generation} (\textbf{SDG}) by leveraging a \textit{taxonomy-driven approach} rather than relying on traditional random sampling methods.\\
While manually curated data samples embedded in the taxonomy’s leaf nodes can be used for direct instruction tuning, prior research suggests that a large volume of high-quality instruction data is necessary for improving instruction-following capabilities in LLMs.\vspace{14pt}\\
Existing SDG methods attempt to scale synthetic data generation using teacher models, but they suffer from mode collapse — over-representing the dominant patterns of the teacher model while neglecting diverse or less common instruction types.\vspace{14pt}\\
This limitation arises from random selection of seed examples, which results in prompts that reflect an "average" of the dataset rather than task-specific examples. Consequently, the teacher model generates synthetic data that predominantly aligns with its dominant distribution while ignoring the long-tail of diverse or nuanced tasks. To overcome this, LAB replaces random selection with taxonomy-driven sampling, ensuring that data generation is targeted at the level of individual leaf nodes. This approach guarantees balanced representation across different instruction types, preventing bias toward a subset of commonly occurring prompts.\vspace{14pt}\\
LAB introduces two new SDG methods based on this taxonomy-guided framework:
\begin{enumerate}
    \item \textbf{Skills Generation}: this method uses the task examples stored in the leaf nodes to generate a larger dataset using the open-source \textbf{Mixtral-7x8B model}. By focusing on specific skill categories, LAB ensures better diversity and task-specific representation in the synthetic data.  
    \item \textbf{Knowledge Generation}: unlike prior approaches that rely on a teacher model’s internal knowledge, LAB’s knowledge generation method still employs \textbf{Mixtral-7x8B}, but without depending on pre-existing knowledge stored in the model. This approach mitigates biases introduced by the teacher model’s knowledge limitations while maintaining control over the source material used for synthetic data creation.  
\end{enumerate}
By structuring data generation around the taxonomy, \textbf{LAB improves both the diversity and quality} of synthetic instruction-tuning datasets, addressing key weaknesses in traditional SDG pipelines.\vspace{14pt}\\
\textbf{Skill Generation}\\
Skills-SDG follows a structured, multi-stage approach to generate diverse, high-quality instructional data using Mixtral-7x8B. Four specialized prompts guide the teacher model in different roles:  
\begin{enumerate}  
    \item \textbf{Instruction Generation}: the model generates diverse, well-structured instructions by systematically exploring taxonomy nodes.  
    \item \textbf{Evaluating Instructions}: instructions are filtered for relevance, safety, and feasibility, ensuring only high-quality queries proceed.  
    \item \textbf{Generating Responses}: responses are tailored per domain, ensuring creativity for writing tasks and precision for STEM subjects.  
    \item \textbf{Evaluating Instruction-Response Pairs}: a rating system filters out inaccurate or off-topic samples, ensuring high-quality data.  
\end{enumerate}
\textbf{Knowledge Generation}\\
Knowledge-SDG addresses SDG limitations by grounding data generation in external sources like manuals and books, reducing hallucinations. Unlike conventional SDG, this method ensures factual accuracy by integrating structured examples with external knowledge. The teacher model evaluates content to maintain reliability, making this approach particularly valuable for specialized domains.  

\subsection{What's under the hood}
Fine-tuning large language models (LLMs) on consumer hardware presents significant challenges due to high computational and memory demands. To address these challenges, we leverage the ILAB framework, which integrates several optimization techniques: Synthetic Data Generation (SDG), Model Quantization, Low-Rank Adaptation (LoRA\cite{hu2021loralowrankadaptationlarge}), and Quantized LoRA (QLoRA\cite{dettmers2023qloraefficientfinetuningquantized}). These methods work together to enable efficient training and adaptation of LLMs on consumer-grade GPUs.

\subsubsection{Synthetic Data Generation (SDG)}
SDG enhances the training process by generating high-quality synthetic data, reducing the dependence on large real-world datasets. This not only helps in domain adaptation but also mitigates data scarcity issues, improving model generalization.

\subsubsection{Model Quantization}
Model quantization reduces the precision of model parameters (e.g., from 16-bit floating point to 8-bit integers), significantly lowering memory requirements and computational costs. This enables larger models to fit within the limited VRAM of consumer GPUs while maintaining near-original performance.

\subsubsection{Low-Rank Adaptation (LoRA)}
LoRA fine-tunes LLMs efficiently by introducing low-rank matrices into selected layers instead of updating all model parameters. This dramatically reduces the number of trainable parameters, making fine-tuning feasible on constrained hardware.

\subsubsection{Quantized LoRA (QLoRA)}
QLoRA combines the benefits of quantization and LoRA by applying quantization techniques to the base model while training LoRA adapters on top. This further reduces memory consumption, allowing large-scale models to be fine-tuned even on GPUs with limited VRAM.

\subsubsection{How These Techniques Work Together}
By combining these methods, ILAB creates a highly efficient fine-tuning pipeline.
\begin{itemize}
    \item SDG provides a diverse training dataset, enhancing model robustness
    \item Model quantization compresses the base model, reducing memory footprint
    \item LoRA then enables efficient fine-tuning without modifying the entire model, and
    \item QLoRA further optimizes this by applying quantization to the base model while keeping adapter layers in higher precision.
\end{itemize}
This synergy allows consumer hardware, such as NVIDIA RTX-series GPUs or Apple Silicon chips, to fine-tune large-scale LLMs that would otherwise require enterprise-level infrastructure.

\subsection{Output Model: GGUF Format with Llama CPP Runner}\label{gguf}
After fine-tuning the large language model (LLM) using the ILAB framework, the resulting model is saved in the GGUF (GPT-Generated Unified Format) format.\\
GGUF is a highly efficient format designed for easy deployment and fast inference, ensuring that the model can be loaded and run with minimal overhead on a variety of hardware configurations. This format is particularly well-suited for use with the Llama CPP runner, which is a lightweight inference engine designed for high-performance model execution.\\
One of the key advantages of the GGUF format, when paired with the Llama CPP runner\footnote{https://github.com/ggerganov/llama.cpp}, is its seamless compatibility with Apple's hardware ecosystem. Apple Silicon (M1, M2, and future generations) is considered a first-class citizen in this setup, benefiting from optimized performance through the use of several native technologies, including ARM NEON, Accelerate, and Metal frameworks.\\
This optimized execution on Apple Silicon enables efficient use of consumer hardware, making it possible to run the fine-tuned model with minimal latency and power consumption, even on relatively modest devices.

\pagebreak

\section{Our Case Study}
Our main goal was to fine-tune a Large Language Model (LLM) using consumer hardware. We chose to focus on creating a model which could help us write Jolie\footnote{https://www.jolie-lang.org/} code. We started by looking for tools that would help us achieve this task. We found IBM's InstructLAB\footnote{https://instructlab.ai/}, which is still in the early stages of development.\vspace{14pt}\\
After an initial phase of studying the tool's documentation, we began by installing the tool.

\subsection{Installing ILAB}
ILAB is essentially a Python package, so it can be installed using pip in your virtual environment with the following command:
\begin{center}
    \texttt{pip install instructlab}
\end{center}
During the installation process, you need to set various parameters to enable GPU acceleration and other features. You can set up all the necessary parameters in your environment by running the following command:
\begin{center}
    \texttt{ilab config init}
\end{center}
Our main machine for this project was an \textit{M1 Pro MacBook Pro 14}, with 16GB of RAM. We also used our department's HPC for various tests before settling on the MacBook.\vspace{14pt}\\
After installing and setting everything up, we created the taxonomy.

\subsection{Creating the Taxonomy}
ILAB's taxonomy consists of different "leaves" called QnA (Questions and Answers) in YAML format. Each leaf covers a single topic.\\
These files are quite simple. An example of our Introduction to Jolie QnA is shown below:
\begin{center}
    \includegraphics[width=0.9\textwidth]{img/qna.png}
\end{center}
We used Jolie's documentation\footnote{https://docs.jolie-lang.org/v1.13.x-git/introduction/index.html} to create QnAs for every part of the language's documentation.\\
ILAB provides a tool to validate the different QnAs using the following command:
\begin{center}
    \texttt{ilab taxonomy diff}
\end{center}
Once we created all the QnAs for the documentation, we used ILAB to convert the taxonomy into datasets.

\subsection{Dataset Generation}
This is where the SDG parts come into play. ILAB uses an instructor LLM to generate datasets by expanding the taxonomy into larger text files, as discussed in section [\ref{tax-sdg}]. In our case, we used \textit{Mixtral-8x7B} as the teacher model — the default teacher model for ILAB — which can be downloaded using the following command:
\begin{center}
    \texttt{ilab model download --hf-token=*}
\end{center}
Generating datasets on our Apple machine was, to say the least, \textit{buggy}. We chose to use the HPC to run this task, which took almost 60 hours on an RTX 2080. If the tool had worked properly, we could have run it on our own machine as well.\\
To generate the datasets, we used the following command:
\begin{center}
    \texttt{ilab data generate --pipeline simple}
\end{center}
We can now move on to the training phase.

\subsection{Training the Base Model}
Once the datasets were ready, we used the following command to start training the base model:
\begin{center}
    \texttt{ilab model train --pipeline simple --local --num-epochs * --iters * --optimize-memory}
\end{center}
We ran this command multiple times to create several models with different iteration counts. We chose not to change the number of epochs — always set to 1 — because the process was somewhat \textit{buggy}. For our case study, we settled on 100, 250, 500, and 1000 iterations.\vspace{14pt}\\
Once all the models were created and ready to use, the final step was to convert them to \textit{.gguf} from \textit{safetensors} format to quantize them.

\subsection{Model Quantization}
This step was necessary in order to \textit{serve} the models, as there is currently no way to run safetensors files on Apple machines. The only available runner is \textbf{LLama.cpp}, which requires a .gguf file [\ref{gguf}]. For each conversion, we used the following command:
\begin{center}
    \texttt{ilab model convert --model-name jolieLLM}
\end{center}
The last, and most interesting, step was to chat with the newly created models.

\subsection{Serving and Chatting with the Models}
ILAB allows you to chat with the model by serving it. You can do so using the following command:
\begin{center}
    \texttt{ilab model serve --model-path ./jolieLLM.gguf}
\end{center}
Once the model is being served, you can chat with it using the following command:
\begin{center}
    \texttt{ilab model chat}
\end{center}

\section{Evaluation of the models}
Once we got all the different models ready, we moved on to compare them to see how they perform as the iteration count increases. We opted for a straightforward method, which can be summarized with:
\begin{itemize}
    \item We prepared five different text prompts [\ref{prompts}], with different domain coverage (theorical, technical, hybrid and coding aspects of the language)
    \item For every model -- even the base one -- we asked the same prompt three times and saved their responses
    \item All of us gave a grade from 0 to 5 to every response
\end{itemize}
After this process, we took all the data and merged them into different plots, to show the behaviour of the models and their perfomance.

\subsection{Prompts}\label{prompts}
\textbf{Prompt 1}\\
How does Jolie handle REST API's? Can you explain how does the language work when implementing them?\vspace{14pt}\\
\textbf{Prompt 2}\\
What's the difference between processes and sessions in Jolie? Can you expand a bit more on the sessions?\vspace{14pt}\\
\textbf{Prompt 3}\\
How does Jolie handle database connections? Can you explain to me how that works and give me some practical examples on how to use them?\vspace{14pt}\\
\textbf{Prompt 4}\\
Can you give me the code for two Jolie programs that communicate with eachother. I want to send an "Hello World" message from one to the other.\vspace{14pt}\\
\textbf{Prompt 5}\\
Help me write the code for a calculator in Jolie. I want my calculator to be able to do basic functions, like sum, subtraction, divide, multiply. Give me the code for the Calculator and the code to try and communicate with it.

\subsection{Results}
The process will be the same for every plot. We took every grade we gave for the prompt and made an average.
\begin{center}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/model-0-grades.png}
    \end{minipage} 
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/model-100-grades.png}
    \end{minipage}
    
    \vspace{1em}

    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/model-250-grades.png}
    \end{minipage} 
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/model-500-grades.png}
    \end{minipage}

    \vspace{2em}
    
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/model-1000-grades.png}
    \end{minipage} 
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/plots/mean-performance-comparison.png}
    \end{minipage}

    \vspace{2em}

    \begin{center}
        \includegraphics[width=1\textwidth]{img/plots/performance-comparison.png}
    \end{center}
\end{center}
The first five plots represent the performance of the models based on the given prompt.\\
We expected a linear growth, but we can clearly see a drop of precision with the 250 iteration model.\\
We can observe an overall improvement by looking at the mean performance plot, where the linear progression can be seen.\\
The last plot shows a comparison between all the prompts across the different models.

\section{Bonus: a new model with 15K iterations}
In the ILAB documentation, we encountered a particularly valuable resource: a Colab Notebook designed to facilitate the demonstration of the model fine-tuning process. This notebook provided a clear, step-by-step illustration of the underlying operations that occur when the command:
\begin{center}
    \texttt{ilab model train}
\end{center}
is executed. The practical utility of this resource cannot be overstated, as it allowed us to gain a deeper understanding of the internal workings of the tool, particularly during the training phase. Observing the process in action enabled us to better comprehend the intricacies of model refinement, including the impact of different hyperparameters and the iterative nature of the training process.\vspace{14pt}\\
Moreover, leveraging this notebook proved instrumental in enhancing the quality and precision of our model. By executing the fine-tuning process on a more powerful GPU (an Nvidia T4 provided by Google), we were able to accelerate the training significantly.\\
This not only improved the efficiency of the model development cycle but also empowered us to perform more extensive training, ultimately achieving a model with 15,000 iterations.\vspace{14pt}\\
The 15k iteration cap was chosen because the training loss after this number was starting to grow again.\\
We did the same grading as before for this new model, and got these results:
\begin{center}
    \includegraphics[width=0.8\textwidth]{img/plots/15k-mean-performance-comparison.png}
\end{center}
\begin{center}
    \includegraphics[width=0.8\textwidth]{img/plots/15k-performance-comparison.png}
\end{center}
It is pretty clear that with more iterations - and with more computing power - you can achieve better results.\\
Of course this goes beyond our initial goal, but we wanted to see how the tool would perform with more power.

\section{Trying to visualize the attention mechanism}
In the context of transformers and attention mechanisms, attention refers to how the model decides which parts of the input sequence to focus on when making predictions. It's a key feature of models like GPT-2, BERT, and others, enabling them to understand relationships between words (or tokens) in a sentence, regardless of their position.\\
In more technical terms, attention allows the model to weigh the importance of different words in a sequence relative to each other.\\
For example, in the sentence "The cat sat on the mat," when predicting the word "sat," the model may focus on "cat" because they are strongly related, while ignoring less relevant words like "on" or "the."\vspace{14pt}\\
It could have been interesting to measure the attention maps of our models while performing inference, in order to see if the model is hallucinating or understands the concept of what he is asked.\\
Using the \texttt{shap} library we were able to measure the attention on a standard not quantized LLM (like GPT2). When we tried to do the same with our models, we encountered some issues.\vspace{14pt}\\
Measuring (or "reading out") attention weights in a transformer is normally done by having the model output its intermediate activations. In full-precision models, this is straightforward, but many quantization methods - especially those implemented for inference efficiency like bitsandbytes' 4-bit (like ours) or 8-bit quantization - fuse operations and replace standard linear layers with custom kernels that do not necessarily expose the intermediate attention matrices.\vspace{14pt}\\
In other words, while the overall architecture (and even its outputs) remains the same, the very purpose of quantization is to optimize the forward pass (both speed - and memory-wise) by ``compressing'' the weights and often fusing computations. This usually means that the intermediate values (such as the un-normalized attention scores) aren’t available in an easy-to-extract form.\\
While it isn’t theoretically ``impossible'' to measure attention on a quantized model, the way current quantization libraries (such as bitsandbytes) are implemented means that the necessary intermediate activations (the attention weights) are typically not available. 
To make it work you’ll likely need to either work with a full-precision model (not our case) or design a custom solution that "dequantizes" or bypasses the fused operations at the point where attention is computed (what we tried).\vspace{14pt}\\
Assuming you can load a full-precision copy of your model - even if you originally used bitsandbytes to run inference with quantized weights - you can run a "full-model" forward pass and then apply SHAP on that version. In practice, this means \textbf{dequantizing a quantized model}.\\
Bitsandbytes also provides a method such as \texttt{model.dequantize()} that converts the quantized weights back to full precision. If you use that method, you obtain a dequantized model whose architecture mirrors that of the original model. In this full-precision copy, you can enable \texttt{output\_attentions} and then compute SHAP explanations over the attention outputs.\vspace{14pt}\\
Unfortunately, we are not able to use a machine with that much GPU VRAM, so while trying to make it work we encountered different \texttt{Out of Memory} errors, which blocked us from going forward.\vspace{14pt}\\
With more powerful machines it could have been theorically possible to plot attentions maps!

\section{Future work}
While our results demonstrate the feasibility of fine-tuning LLMs on consumer hardware, several areas remain open for further research and improvement.\\
First, starting with a stronger base model — one that is both open-source and optimized for coding tasks — could significantly enhance performance. Additionally, employing more powerful consumer hardware, such as higher-end Apple Silicon chips or consumer-grade GPUs with increased VRAM, would reduce training time and allow for more iterations, improving model refinement.\vspace{14pt}\\
Another crucial avenue for future work is improving the taxonomy used for instruction tuning. The current taxonomy is relatively limited, and expanding it with a broader set of high-quality examples and additional categories would substantially enhance the model's capabilities.\vspace{14pt}\\
As ILAB is still in an early development phase, we anticipate that future iterations of the tool will introduce optimizations, making it possible to achieve superior results even with the same hardware.\vspace{14pt}\\
Finally, a more in-depth exploration of visualization techniques for model attention mechanisms could provide deeper insights into how fine-tuned models process information. Overcoming the challenges associated with quantized models’ attention weight extraction would be a valuable step in this direction.

\section{Conclusions}
This work demonstrates that fine-tuning large language models using consumer hardware is feasible when leveraging optimized training techniques such as model quantization, low-rank adaptation, and taxonomy-driven synthetic data generation.\\
By utilizing ILAB, we were able to develop and evaluate a model trained with minimal computational resources, highlighting both the potential and current limitations of this approach. While challenges remain — particularly in terms of hardware constraints and dataset quality — our findings suggest that future advancements in instruction tuning frameworks and hardware capabilities will further bridge the gap between high-performance model training and accessibility for a broader audience.

\pagebreak

\bibliography{ref.bib}
\end{document}